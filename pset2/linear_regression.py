import sys
import os
LFDpath = "/".join(os.path.dirname(__file__).split('/')[0:-1])
if not LFDpath in sys.path:
    sys.path.append(LFDpath)
from pset1.PLA import *     # this should include numpy, and matplotlib

def LG(x, y):
    """
        x: Each row is [x1, x2, ... xn]. Total of N rows.
        y: Each row is the value corresponding to y=f(x1,x2, ... ,xn). Total of N rows.
    """
    N = x.shape[0]
    X = np.ones((N, x.shape[1]+1))
    X[:, 1:] = x
    Xdagger = np.dot(np.linalg.inv(np.dot(X.T, X)), X.T)
    weights = np.dot(Xdagger, y)
    return weights

def question5_6():
    trials = 1000
    N = 100.0
    Ein_array = np.ones((trials,1))*np.NAN
    target_fn_array = np.ones((trials, 2))*np.NAN
    w_array = np.ones((trials, 3))*np.NAN

    for i in xrange(trials):
        target_fn = generate_targetFn()
        data = generate_dataPoints_fromFn(N, target_fn)
        w = LG(data[:, 0:2], data[:, 2])
        y_out = np.array([w[0]+w[1]*row[0]+w[2]*row[1] for row in data])
        E_in = (N - np.count_nonzero(np.sign(y_out) == np.sign(data[:,2])))/N

        Ein_array[i] = E_in
        target_fn_array[i, :] = target_fn
        w_array[i, :] = w

    print "Problem 5: Average in-sample error is %0.3f" % (Ein_array.mean())

    # plot the last regression-line classification boundary and the data points, also the targetfn.
    """
    plot_fn_and_data(target_fn, data, 1)
    LG_fn = (-w[1]/w[2], -w[0]/w[2])
    plot_fn_and_data(LG_fn, data, 2)
    """

    # Now find the out-of-sample error by generating 1000 fresh points per function
    N = 1000.0
    Eout_array = np.ones((trials,1))*np.NAN

    for i in xrange(trials):
        data = np.ones((N, 4))
        # each row contains x[0], x[1], x[2], y
        data[:, 1:] = generate_dataPoints_fromFn(N, target_fn_array[i, :])
        regression_out = np.array([np.dot(w_array[i,:], x) for x in data[:, 0:3]])
        Eout_array[i] = (N - np.count_nonzero(np.sign(regression_out) == np.sign(data[:,3])))/N

    print "Problem 6: Average out-sample error is %0.3f" % (Eout_array.mean())

def question7():
    """
        Use N=10 training data points. Get weights with Linear Regression, use those
        weights to initialize a PLA and see how long it takes to converge, over 1000 runs.
    """

    N = 10.0
    trial = 5000
    convergence_steps = np.ones((trial,1))*np.NAN
    for i in xrange(trial):
        target_fn = generate_targetFn()
        data = generate_dataPoints_fromFn(N, target_fn)
        w = LG(data[:, 0:2], data[:,2])
        classifier = PLA(data, weights = w)
        convergence_steps[i] = classifier.train()
    print "Problem 7: Average steps took for classifier convergence: %0.3f" % (convergence_steps.mean())

def nonlinear_target_fn_genData(N, p_noise):
    """
        The target function for 8, 9, and 10 is:
            f(x1, x2) = sign(x1^2 + x2^2 - 0.6)
        we generate the data from this target function, with a probability of p_noise of
        flipping the output

        Output:
            data: Each row is (1, x1, x2, y)
    """
    data = np.ones((N, 4))

    i = 0
    while i < N:
        (x1,x2) = np.random.uniform(-1, 1, 2)
        s = np.sign(x1*x1 + x2*x2 - 0.6)
        if s == 0:
            continue
        else:
            s = np.random.choice([s,-s], p=[1-p_noise, p_noise])
            data[i,1:] = [x1,x2,s]
            i += 1
    return data

def question8():
    """
        Use N = 1000 training points, with feature vector (1,x1,x2) for linear regression on
        data generated by the nonlinear target-function.

        Run experiment 1000 times and find average Ein.
    """
    N = 1000.0
    trial = 1000
    p_noise = 0.1
    Ein_array = np.ones((trial,1))*np.NAN
    for i in xrange(trial):
        data = nonlinear_target_fn_genData(N, p_noise)  
        # row of data: [1, x1, x2, y], LG takes [x1,x2] and [y]
        w = LG(data[:, 1:3], data[:,3])
        y_out = np.array( [np.dot(w, row[0:3]) for row in data] )
        Ein_array[i] = (N - np.count_nonzero(np.sign(y_out) == np.sign(data[:, 3])))/N

    print "Problem 8: Average in-sample error is %0.3f" % (Ein_array.mean())

def question9():
    """
        Use N = 1000 training points, and convert to feature vector (1, x1, x2, x1*x2, x1^2, x2^2)
        for linear regression on data generated by the nonlinear target-function.

        Find one set of weights, then compare them to the alternative ones on randomly generated
        test set (100 points) to see which one is closer.
    """
    N = 1000.0
    p_noise = 0.1
    training_data = nonlinear_target_fn_genData(N, p_noise)
    # original data is [1, x1, x2, y].
    # training_x is (1, x1, x2, x1*x2, x1^2, x2^2)
    training_x = np.array([[1, row[1], row[2], row[1]*row[2], row[1]**2, row[2]**2] for row in training_data])
    w = LG(training_x[:, 1:], training_data[:, -1])

    # choices of g, weights for the input components
    w_others = np.vstack((w, np.array([-1, -0.05, 0.08, 0.13, 1.5, 1.5]),\
                         np.array([-1, -0.05, 0.08, 0.13, 1.5, 15]),\
                         np.array([-1, -0.05, 0.08, 0.13, 15, 1.5]),\
                         np.array([-1, -1.5, 0.08, 0.13, 0.05, 0.05]),\
                         np.array([-1, -0.05, 0.08, 1.5, 0.15, 0.15])))

    trial = 50
    N_test = 100
    agreement = np.zeros((trial, 5))
    y = np.zeros((N_test, 6))
    for i in xrange(trial):
        test_data = nonlinear_target_fn_genData(N_test, p_noise)
        test_x = np.array([[1, row[1], row[2], row[1]*row[2], row[1]**2, row[2]**2] for row in test_data])
        for j in xrange(6):
            y[:, j] = np.array([np.dot(w_others[j], row) for row in test_x])
        # compare the results against each other
        for j in xrange(5):
            agreement[i, j] = np.count_nonzero(np.sign(y[:,0]) == np.sign(y[:,j+1]))

    agreement = agreement.mean(axis=0)
    print "Problem 9: Choice %d agrees with linear regression hypothesis the best." % (np.argmax(agreement)+1)

def question10():
    """
        Generate a hypothesis to the nonlinear target fn with N=1000 training points, using
        nonlinear feature vector.

        Then test this hypothesis on 1000 runs of 1000-points test set and find the average of this
        out-sample performance.
    """
    N = 1000.0
    p_noise =0.1
    trials = 1000
    training_data = nonlinear_target_fn_genData(N, p_noise)
    # original data is [1, x1, x2, y].
    # training_x is (1, x1, x2, x1*x2, x1^2, x2^2)
    training_x = np.array([[1, row[1], row[2], row[1]*row[2], row[1]**2, row[2]**2] for row in training_data])
    w = LG(training_x[:, 1:], training_data[:, -1])

    Eout_array = np.ones((trials, 1))*np.NAN
    for i in xrange(trials):
        test_data = nonlinear_target_fn_genData(N, p_noise)
        test_x = np.array([[1, row[1], row[2], row[1]*row[2], row[1]**2, row[2]**2] for row in test_data])
        y_out = np.array([np.dot(w, row) for row in test_x])
        Eout_array[i] = (N-np.count_nonzero(np.sign(y_out) == np.sign(test_data[:, -1])))/N

    print "Problem 10: Out-of-sample error is %0.3f" % (Eout_array.mean())


        


            








